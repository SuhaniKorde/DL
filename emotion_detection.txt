# 1Ô∏è‚É£ Import Libraries
import tensorflow as tf
from tensorflow.keras import layers, models, preprocessing
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files
import os

# 2Ô∏è‚É£ Upload the ZIP file manually (if not already done)
# Run this cell if you haven't uploaded the file yet
# uploaded = files.upload()

# 3Ô∏è‚É£ Unzip the uploaded dataset
!unzip "archive (1).zip" -d /content/dataset

# 4Ô∏è‚É£ Check the folder structure
!ls /content/dataset

# üëá Adjust these paths if needed after seeing the folder names above
train_dir = '/content/dataset/train'
test_dir = '/content/dataset/test'

# 5Ô∏è‚É£ Data Preprocessing
train_datagen = preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = preprocessing.image.ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(32, 32),
    batch_size=32,
    color_mode='grayscale',
    class_mode='categorical'
)

test_data = test_datagen.flow_from_directory(
    test_dir,
    target_size=(32, 32),
    batch_size=32,
    color_mode='grayscale',
    class_mode='categorical'
)

# 6Ô∏è‚É£ Build CNN Model
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,1)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),
    layers.Dropout(0.25),

    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(train_data.num_classes, activation='softmax')
])

# 7Ô∏è‚É£ Compile Model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# 8Ô∏è‚É£ Train Model
history = model.fit(
    train_data,
    validation_data=test_data,
    epochs=25,
    verbose=1
)

# 9Ô∏è‚É£ Plot Accuracy and Loss
plt.figure(figsize=(12,4))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# üîü Evaluate Model
test_loss, test_acc = model.evaluate(test_data)
print(f"\n‚úÖ Test Accuracy: {test_acc:.2f}")
print(f"‚úÖ Test Loss: {test_loss:.4f}")

# 1Ô∏è‚É£1Ô∏è‚É£ Predict Function
def predict_emotion(img_path):
    img = preprocessing.image.load_img(img_path, target_size=(32,32), color_mode='grayscale')
    img_array = preprocessing.image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0
    prediction = model.predict(img_array)
    class_labels = list(train_data.class_indices.keys())
    predicted_class = class_labels[np.argmax(prediction)]
    print(f"\nüéØ Predicted Emotion: {predicted_class}")

# 1Ô∏è‚É£2Ô∏è‚É£ Upload and Test Prediction Image
print("\nüì§ Upload an image to test prediction:")
uploaded = files.upload()

for fn in uploaded.keys():
    img_path = f'/content/{fn}'
    predict_emotion(img_path)

# 1Ô∏è‚É£3Ô∏è‚É£ Save Model
model.save('/content/emotion_recognition_cnn.h5')
print("\n‚úÖ Model saved as 'emotion_recognition_cnn.h5'")
